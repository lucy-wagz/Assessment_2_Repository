{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kt2f0PB5x0E"
      },
      "source": [
        "# Introduction\n",
        "Student ID Number: 230002756\n",
        "\n",
        "GitHub Repository: https://github.com/lucy-wagz/Assessment_2_Repository\n",
        "\n",
        "This assessment intends to showcase my exploration into Python. With some previous python knowledge from highschool and private instruction, I utilized previously learned skills in a new context: Spatial Analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cXOryPZ5rmE"
      },
      "source": [
        "# Part 1: Python Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztEYEbH96E7d"
      },
      "source": [
        "## 1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1wwlFdD69wt"
      },
      "outputs": [],
      "source": [
        "10+20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KOn6AZn6_gO"
      },
      "outputs": [],
      "source": [
        "# in Google Colab, pressing the D key 2 times does not delete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGPJ-wKo7Uz-"
      },
      "source": [
        "## 1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRtpwtbP7BGt"
      },
      "outputs": [],
      "source": [
        "# list of numbers\n",
        "numbers = [80,90,95,87,82]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V36R9sK8OKH"
      },
      "outputs": [],
      "source": [
        "# use the workbook instructions and the def (calculate_average) for the list of numbers to find the average\n",
        "# in order to find the average, we must write a function finding both the sum of the numbers and a functions finding the amount of numbers\n",
        "# the average = total / amount\n",
        "\n",
        "def calculate_average (num_list):\n",
        "    total = sum(num_list)\n",
        "    count = len(num_list)\n",
        "    average = total / count\n",
        "    return average\n",
        "\n",
        "# then, we must print the result using the print function\n",
        "\n",
        "result = calculate_average(numbers)\n",
        "print(\"The average is:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAdfuC6T9Yye"
      },
      "source": [
        "## 1.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlN1uZ7b9ivW"
      },
      "source": [
        "The pandas library is necessary to work with spreadsheets/CSV files, geopandas (gpd) is so that we can work with geographical data (points, polygons, spatial operations) in this case, we are assessing a world cities shapefile and a CSV file about world cities as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyR9wlMh94XN"
      },
      "source": [
        "## 1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyoESO-p9-7d"
      },
      "outputs": [],
      "source": [
        " # correct code:\n",
        "\n",
        "name = 'Dave'\n",
        "dogs = 0\n",
        "print('My name is', name, 'and I own', dogs, 'dogs.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92w8cU4b-puF"
      },
      "source": [
        "Indentation only makes sense when the code is \"inside\" of something (e.g., an if True statement) or part of a separate block/ another function or loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JerSk38F-59W"
      },
      "source": [
        "## 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGs2g5nL-8t0"
      },
      "outputs": [],
      "source": [
        "a = \"Taco \"\n",
        "b = \"time\"\n",
        "c = a + b\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DGGSrtT_Cd0"
      },
      "source": [
        "Now in the following code cell, I created a script where I print string and integer variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmsMfhac--qt"
      },
      "outputs": [],
      "source": [
        "a = \"Lucy\"\n",
        "b = \" is a student at St Andrews.\"\n",
        "c = a + b\n",
        "print (c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTtYg6X7_NiE"
      },
      "source": [
        "## 1.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVWdYHrd_0Wc"
      },
      "source": [
        "The provided code was incorect because of the indentation of the code blocks. Because the line'temperature = 10' is not nested within another block, there is no need for its indentation.\n",
        "\n",
        "Below is the correct indentation for the code block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VWVYZJW_hyV"
      },
      "outputs": [],
      "source": [
        "season = \"Winter\"\n",
        "temperature = 10\n",
        "\n",
        "if season == \"Winter\":\n",
        "\n",
        "     if temperature > 7:\n",
        "         print(\"No need for winter jacket!\")\n",
        "\n",
        "     else:\n",
        "         print(\"It might be cold! Wear a proper jacket!\")\n",
        "\n",
        "elif season == \"Summer\":\n",
        "\n",
        "     if temperature > 20:\n",
        "         print(\"It's warm! Time to wear shorts!\")\n",
        "\n",
        "     else:\n",
        "         print(\"Well this is Finland, better wear long trousers!\")\n",
        "else:\n",
        "     print(\"Check the weather forecast!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ZdAOgqATQU"
      },
      "source": [
        "In the following code cell, I have created a similar script with several nested *if* conditions cimilar to the example above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlZX7clIAasL"
      },
      "outputs": [],
      "source": [
        "season = \"Spring\"\n",
        "temperature = 18\n",
        "\n",
        "if season == \"Spring\":\n",
        "\n",
        "    if temperature > 18:\n",
        "        print(\"You don't need a sweater today.\")\n",
        "\n",
        "    else:\n",
        "        print(\"You should wear a light sweater just in case.\")\n",
        "\n",
        "elif season == \"Autumn\":\n",
        "\n",
        "    if temperature < 15:\n",
        "        print(\"It's getting cold, you should put on another layer\")\n",
        "\n",
        "    else:\n",
        "        print(\"You dont necessarily need another layer, but it would be good to have\")\n",
        "\n",
        "else:\n",
        "    print(\"Check the weather forecast.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqLQ8H4BDX39"
      },
      "source": [
        "## 1.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4fxx2_qAcWV"
      },
      "outputs": [],
      "source": [
        "def celsius_to_fahr(temp):\n",
        "    return 9 / 5 * temp + 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWOrlf4iDpTW"
      },
      "source": [
        "We know that the 1 mile is equal to 1.60934 kilometers, so creating a function will utilize (miles x kilometers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B8tkpwSDlzF"
      },
      "outputs": [],
      "source": [
        "# this defines what the name will correlate to in the printed product\n",
        "\n",
        "def miles_to_kms(miles):\n",
        "    return miles * 1.60934"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cu1sN4YD05W"
      },
      "outputs": [],
      "source": [
        "# in this case, I illustrated what 1,5, and 10 miles is in kilometers\n",
        "\n",
        "print(miles_to_kms(1))\n",
        "print(miles_to_kms(5))\n",
        "print(miles_to_kms(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXqK1vjcFxbL"
      },
      "source": [
        "## 1.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx9ca5YpEI2G"
      },
      "outputs": [],
      "source": [
        "# n will be the number of the multiplication table (e.g., a table for 3s)\n",
        "# the range (1,11) means that the loop will start at 1 and go to 10, running the code 10 times\n",
        "\n",
        "def multiplication_table(n):\n",
        "    for i in range(1,11):\n",
        "        print(n,\"x\",i,\"=\",n*i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXVaBQCqG4hs"
      },
      "outputs": [],
      "source": [
        "# table for 3\n",
        "\n",
        "print(\"Multiplication table for 3:\")\n",
        "multiplication_table(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMRDqR6JG9n8"
      },
      "outputs": [],
      "source": [
        "# table for 12\n",
        "\n",
        "print(\"Multiplication table for 12:\")\n",
        "multiplication_table(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnB9izK0HHzc"
      },
      "source": [
        "## 1.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cElTKNKfHAL0"
      },
      "outputs": [],
      "source": [
        "# I altered the example to showcase new lunch components\n",
        "\n",
        "lunch = [\n",
        "    'Chips',\n",
        "    'Chips',\n",
        "    'Sandwich',\n",
        "    'Wrap',\n",
        "    'Potato',\n",
        "    'Wrap',\n",
        "    'Sandwich',\n",
        "    'Wrap',\n",
        "    'Tea',\n",
        "    'Potato',\n",
        "    'Potato',\n",
        "    'Coffee'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upGHrl9fHznF"
      },
      "outputs": [],
      "source": [
        "# this code goes through each item in this list and counts how many times the item appears, as seen below\n",
        "\n",
        "counts = {}\n",
        "\n",
        "for item in lunch:\n",
        "    if item not in counts:\n",
        "        counts[item] = 1\n",
        "    else:\n",
        "        counts[item] += 1\n",
        "\n",
        "print(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIPByxh7MkRy"
      },
      "source": [
        "## 1.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXcY5d7CRHkr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6g0E3bdMLD5"
      },
      "outputs": [],
      "source": [
        "# in ArcGIS, the code was saved to my H drive and uploaded from there:\n",
        "  # H:\\GG3209\\week_5_Python\\Latest_earthquake_world.csv\n",
        "# I then imported pandas as seen below\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# however, Colab must have files uploaded from the drive, so this is my pathway:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/python_basics/Latest_earthquake_world.csv\")\n",
        "df.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euiqaQRyRtUB"
      },
      "source": [
        "# Part 2: Pandas and NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK_sI3JdUT9l"
      },
      "source": [
        "## 2.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CnLRxz8R14S"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7fZ0a74UXiM"
      },
      "source": [
        "## 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEG1JjhyUWDz"
      },
      "outputs": [],
      "source": [
        "# the function arr1 = np.ones(10) creates an array entirely of 1s\n",
        "  # the argument = 10 which means in this case there are 10 1s\n",
        "\n",
        "# the function arr2 = np.ones(1,21) creates an array of integers 1 to 20\n",
        "  # the function is classified in parenthesis as (start, stop) meaning that it will start at the nuber 1 and end at the number before 21\n",
        "\n",
        "# the function arr3 = np.ones((5,5), dtype=int) creates a matrix of 5x5 1s\n",
        "  # np.ones((5,5)) is the function for creating a matrix with 5 rows and 5 columns\n",
        "  # dtype = int forces the 1 to be integers instad of floats -- this is necessary because we are indexing 1s, not counting them\n",
        "\n",
        "arr1 = np.ones(10)\n",
        "arr2 = np.arange(1,21)\n",
        "arr3 = np.ones((5,5), dtype = int)\n",
        "print(arr1)\n",
        "print(arr2)\n",
        "print(arr3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNpxsYxFVfIg"
      },
      "source": [
        "## 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvzsIr9IUbPr"
      },
      "outputs": [],
      "source": [
        "# I begin by importing numpy as np\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# similar to the above function, i norder to print arr1, I had to define what it is\n",
        "  # in this case, it prints a 3D matrix of 3x3x3 ()\n",
        "arr1 = np.random.randn(3,3,3)\n",
        "print(arr1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui8veDkKcdm6"
      },
      "outputs": [],
      "source": [
        "# this reshapes the array into a list of 27 randm numbers, as opposed to a 3x3x3 matrix\n",
        "\n",
        "arr2 = arr1.reshape(27,)\n",
        "print(arr2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J83nXJFIcrdC"
      },
      "source": [
        "## 2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7-wjUPNctPx"
      },
      "outputs": [],
      "source": [
        "# this produces 20 evenly spaced numbers between 1 and 10\n",
        "  # (start, end, number of points)\n",
        "\n",
        "arr = np.linspace(1,10,20)\n",
        "print(arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXQPRoSWdCfp"
      },
      "source": [
        "## 2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz7AAFQmdDtB"
      },
      "outputs": [],
      "source": [
        "# arrange: numbers 1-25\n",
        "# reshape: puts this in a 5x5 matrix\n",
        "\n",
        "a = np.arange(1, 26).reshape(5, 5)\n",
        "\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L12NGj5rdHQS"
      },
      "outputs": [],
      "source": [
        "# this indexes a part of the matrix by [row, column], in this instance 3 and 4\n",
        "# row 3, column 4 = 20\n",
        "\n",
        "a[3,4]\n",
        "\n",
        "print(a[3,4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31AH5BYSdJzB"
      },
      "outputs": [],
      "source": [
        "# the colon function (:) indicates all rows between numbers -- in this case, 1-5\n",
        "# after the comma, the 3:5 indicates which colums to pick numbers from -- in this case, 3-5\n",
        "\n",
        "a[1:5, 3:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a48orAK7d8Wx"
      },
      "outputs": [],
      "source": [
        "# an empty (colon only) number within the brackets indicates that an entire row is to be selected, in this case, row 1\n",
        "\n",
        "a[1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu22bo5YeoFi"
      },
      "source": [
        "## 2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1-nnnJ5eo3J"
      },
      "outputs": [],
      "source": [
        "# using the sum function calculates the sum of all numbers in the array\n",
        "\n",
        "a.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBHGMQzWe9ea"
      },
      "source": [
        "## 2.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiaCoaKoe-i6"
      },
      "outputs": [],
      "source": [
        "# in order to calculate the sum of 1 row only, the row must be defined as an axis\n",
        "# for row 1, t would be axis = 1\n",
        "\n",
        "a.sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc9j6_FsfLgC"
      },
      "source": [
        "## 2.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr8uFp4cfeDj"
      },
      "outputs": [],
      "source": [
        "mean_value = a.mean()\n",
        "print(mean_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbDtG4qyfMo6"
      },
      "outputs": [],
      "source": [
        "# the mask must print all values in the array that are greater then 13\n",
        "\n",
        "mask = a > mean_value\n",
        "\n",
        "a[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8nhibv9foOK"
      },
      "source": [
        "## 2.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7TXq9m1fpAi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI8rYaTNfsE7"
      },
      "source": [
        "## 2.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNHc1_nufrbj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv\"\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZbXDlg1f0ni"
      },
      "source": [
        "## 2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbYPa2zMfwHa"
      },
      "outputs": [],
      "source": [
        "# using the function df.shape, we can see how many rows and columns are in the dataset\n",
        "\n",
        "df.shape\n",
        "rows, columns = df.shape\n",
        "print(\"Number of rows:\", rows)\n",
        "print(\"Number of columns:\", columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlpvDqrdhC6B"
      },
      "source": [
        "## 2.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_KxnmJPgfpH"
      },
      "outputs": [],
      "source": [
        "# using the .mean() function I was able to find the mean between the countries, and then I rounded it to 2 decimal places\n",
        "\n",
        "mean_co2 = df['co2_emmission'].mean().round(2)\n",
        "print(\"Mean CO2 emission:\", mean_co2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5RAV23Nh4U2"
      },
      "source": [
        "## 2.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_K3J76ChaUh"
      },
      "outputs": [],
      "source": [
        "# maximum CO2 emissions from one country\n",
        "\n",
        "df['co2_emmission'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mR9G-pQ1h9a8"
      },
      "outputs": [],
      "source": [
        "# .loc selects by index - as we are trying to find which country is responsible for the high CP2 emissions\n",
        "# if maximum emissions are in row 2 (as seen in the table) it can be inferred that Argentina produces the most CO2\n",
        "df.loc[df['co2_emmission'].idxmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90jUOfs7ioJv"
      },
      "outputs": [],
      "source": [
        "max_value = df['co2_emmission'].max()\n",
        "max_row = df.loc[df['co2_emmission'].idxmax()]\n",
        "\n",
        "# I figured out a way to print a more accessible view of the max emissions, country responsble, and food product using the table/previously written code\n",
        "print(\"Maximum CO2 emission:\", max_value)\n",
        "print(\"Country:\", max_row['country'])\n",
        "print(\"Food Category:\", max_row['food_category'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhUhq9JViyZg"
      },
      "source": [
        "## 2.13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXlkes3TjBjZ"
      },
      "outputs": [],
      "source": [
        "# by setting the co2_emmission category > 1000, it illustrates which countries in the dataset match that threshold\n",
        "# using n.unique counts how many different countries fit this criteria and condenses it into 1 number\n",
        "\n",
        "high_emitters = df[df['co2_emmission'] > 1000]['country'].nunique()\n",
        "print(\"Number of countries emitting > 1000 kg CO2 per person, pear year:\", high_emitters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnR3ZHHmj3S0"
      },
      "source": [
        "## 2.14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPDbQ7h6jDnR"
      },
      "outputs": [],
      "source": [
        "# first, I assessed the different items within the 'food_category' column of data\n",
        "\n",
        "df['food_category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4xT7HEckHft"
      },
      "outputs": [],
      "source": [
        "# the question asked us to assess what country consumes the least amount of beef per year, so i selected by item attribute with the following code\n",
        "\n",
        "beef = df[df['food_category'] == 'Beef']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzAJAf_ikYr_"
      },
      "outputs": [],
      "source": [
        "# I then used the .idxmin() function to assess where in the table the least amount of beef was consumed\n",
        "\n",
        "least_beef = beef.loc[beef['consumption'].idxmin()]\n",
        "least_beef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYP9FbZ-kVFF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# this filters so that 'beef' is the only \"food_category\" criteria that shows up\n",
        "# additionally, some of the data was labelled 'BEEF, Beef, or beef' so I ran an aditional function to convert to lowercase\n",
        "beef = df[df['food_category'].str.lower() == 'beef']\n",
        "\n",
        "# this locates the row in which beef consumption is lowest\n",
        "least_beef = beef.loc[beef['consumption'].idxmin()]\n",
        "\n",
        "print(\"Country:\", least_beef['country'])\n",
        "print(\"Consumption:\", least_beef['consumption'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGEyO6lumasS"
      },
      "source": [
        "## 2.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_3V_V5jlW4A"
      },
      "outputs": [],
      "source": [
        "# first I crated a list to only show meat in the 'food_category' column\n",
        "meat_types = ['pork', 'poultry', 'fish', 'lamb & goat', 'beef']\n",
        "\n",
        "# then, I filtered the dataset to only keep the meat types when collecting data from the dataset\n",
        "# the smaller dataframe helps contain the values to only meat products and their corresponding data\n",
        "meat_df = df[df['food_category'].str.lower().isin(meat_types)]\n",
        "\n",
        "# the meat emissions selects the co2 emissions from just the meat dataframe and calculates the sum of all number\n",
        "total_meat_emissions = meat_df['co2_emmission'].sum()\n",
        "\n",
        "print(\"Total emissions from meat products:\", total_meat_emissions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWVF9NLtnOol"
      },
      "source": [
        "## 2.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sORaZrVBnH4c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv\"\n",
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCfTELxcnQF0"
      },
      "outputs": [],
      "source": [
        "# in order to assess emissions of non-meats, we must filter out the previoulsy used meat_types list\n",
        "\n",
        "# the ~df is used as a NOT operator, essentially it flips the validity of the statement so that non-meats are included and meat_types are excluded\n",
        "non_meat_df = df[~df['food_category'].str.lower().isin(meat_types)]\n",
        "\n",
        "# similar to above, we run a .sum() function to find the total emissions from non-meat foods\n",
        "total_non_meat_emissions = non_meat_df['co2_emmission'].sum()\n",
        "\n",
        "print(\"Total non-meat emissions:\", total_non_meat_emissions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.17 - World Cities Final Challenge"
      ],
      "metadata": {
        "id": "7Q8miGMwS9aK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS9QOI4boE1N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ONG8xAVoGL9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# here, I defined 'cities' as the CSV file provided\n",
        "cities = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/final_challenge/world_cities.csv\")\n",
        "\n",
        "# .head() shows the first few rows of the dataframe as to not crowd the notebook with all rows\n",
        "cities.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHujiNKgoHjt"
      },
      "outputs": [],
      "source": [
        "# in order to calculate the population in millions (pop_M), the original population column had to be divided by 1,000,000\n",
        "\n",
        "cities[\"pop_M\"] = cities[\"pop\"] /1_000_000\n",
        "cities.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSq2Ut58orz-"
      },
      "outputs": [],
      "source": [
        "# next, I dopped the now unnecessary 'pop' column as we now have the population displayed in millions\n",
        "\n",
        "cities = cities.drop(columns=[\"pop\"])\n",
        "cities.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_ROjj0IoRAd"
      },
      "outputs": [],
      "source": [
        "# next, i filtered the dataset to show me only cities starting with the first letter of my name, in this case, L for Lucy\n",
        "\n",
        "# through research, I foud that .str.startswtith(\"L\") runs a series of True/False tests on cities to see if they fit the qualification of starting with the letter L\n",
        "cities_with_L = cities[cities[\"city\"].str.startswith(\"L\")]\n",
        "cities_with_L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9bCGYEypNRn"
      },
      "outputs": [],
      "source": [
        "# my chosen city is Lisbon in Portugal, and I achieved this by setting the city category == to Lisbon in the code\n",
        "\n",
        "my_city = cities_with_L[cities_with_L[\"city\"] == \"Lisbon\"]\n",
        "my_city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v49t0sNKpm2Q"
      },
      "outputs": [],
      "source": [
        "# additionally, I printed a sentence illustrating my chosen city and what country it is in.\n",
        "\n",
        "my_city1 = \"Lisbon\"\n",
        "print(\"My chosen city is\", (my_city1), \"-- located in Portugal.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxHSq87_puww"
      },
      "outputs": [],
      "source": [
        "# this function selects the row in the dataset\n",
        "my_city = cities[cities[\"city\"] == \"Lisbon\"].iloc[0]\n",
        "\n",
        "# next, i defined the name of the country for my selected city, which will also be used to define the other 4 most populated cities in the final dataset\n",
        "country_name = my_city[\"country\"]\n",
        "\n",
        "# this function finds other cities that match the same country Lisbon is located in (Portugal)\n",
        "country_cities = cities[cities[\"country\"] == country_name]\n",
        "\n",
        "# this function prints the dataset and sorts it by largest to smallest populations first\n",
        "# the .head(5) selects the top 5 countries without displaying other ones\n",
        "top5_country = country_cities.sort_values(by=\"pop_M\", ascending=False).head(5)\n",
        "top5_country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPV9r8hKtM4y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "folder = \"/content/drive/MyDrive/Colab Notebooks/Assessment_2_230002756\"\n",
        "os.listdir(folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Geopandas and Rasterio"
      ],
      "metadata": {
        "id": "WSz0StQnuAcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.01 (importing data/installs)"
      ],
      "metadata": {
        "id": "MGAqyiXAuRP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyuyqnkAvzdO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5WVQDIyv75O"
      },
      "outputs": [],
      "source": [
        "pip install contextily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnxRhDUjv-83"
      },
      "outputs": [],
      "source": [
        "pip install mapclassify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrGJz5et5Bgl"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define the folder containing the zip files\n",
        "folder_with_zips = '/content/drive/MyDrive/Colab Notebooks/data1/'  # Replace with the path to your drive folder if needed\n",
        "\n",
        "# Ensure the output directory exists\n",
        "output_dir = '/content/drive/MyDrive/Colab Notebooks/data1/'\n",
        "os.makedirs(output_dir, exist_ok=True) # Always good to check the folder exists.\n",
        "\n",
        "# Iterate through the files in the folder\n",
        "for filename in os.listdir(folder_with_zips):\n",
        "    file_path = os.path.join(folder_with_zips, filename)\n",
        "\n",
        "    # Check if the file is a zip file\n",
        "    if filename.endswith('.zip'):\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # Create a folder with the same name as the zip file (without the .zip extension), that is very convenient.\n",
        "            folder_name = os.path.splitext(filename)[0]\n",
        "            folder_path = os.path.join(output_dir, folder_name)\n",
        "            os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "            # Extract the contents of the zip file into the folder\n",
        "            zip_ref.extractall(folder_path)\n",
        "\n",
        "            print(f'Oh look what I got, Unzipped {filename} to {folder_path}')\n",
        "    else:\n",
        "        print(f'Skipping {filename} (Sorry, not a zip file)')\n",
        "\n",
        "print(\"Unzipping completed. Wahoo!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYMzWIizxGfA"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import geopandas as gpd\n",
        "import contextily as ctx\n",
        "import rasterio as rio\n",
        "from rasterio import plot\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2zePclpz52b"
      },
      "source": [
        "## 3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdPucWCKz15T"
      },
      "outputs": [],
      "source": [
        "# these functions locate and read the file\n",
        "\n",
        "lsoa_geo = gpd.read_file('/content/drive/MyDrive/Colab Notebooks/data1/londondata/Census20_LSOA.shp')\n",
        "lsoa_geo.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2"
      ],
      "metadata": {
        "id": "h206QDqMvS39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEjwqvjd0etU"
      },
      "outputs": [],
      "source": [
        "# lsoa_sample is created with intent to filter the dataset to only the following criteria\n",
        "  # LSOA Area Code\n",
        "  # LSOA Area Name\n",
        "  # LSOA Bigger Area\n",
        "  # Population Counts\n",
        "# as seen below\n",
        "\n",
        "lsoa_sample = lsoa_geo [[\"LSOA11CD\", \"LSOA11NM\", \"LSOA11NMW\", \"Pop20\", \"geometry\"]]\n",
        "lsoa_sample.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3"
      ],
      "metadata": {
        "id": "sIQ6MvfFvwod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky75PBcK80Oz"
      },
      "outputs": [],
      "source": [
        "# using .crs shows the Coordinate Reference System of the dataset, in this case it is EPSG:27700 (British National Grid)\n",
        "\n",
        "lsoa_sample.crs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4"
      ],
      "metadata": {
        "id": "fxbZMe9_wCBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKfEPNim9JDW"
      },
      "outputs": [],
      "source": [
        "# the .int function converts the number to a Python integer, wich is done to avoid lengthy outputs (int8 or int64)\n",
        "# after that, the function .shape illustrates the total number of features (rows) in the new, filtered dataset\n",
        "\n",
        "int(lsoa_sample.shape[0])\n",
        "print(\"The number of features is: \", int(lsoa_sample.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5"
      ],
      "metadata": {
        "id": "5Ym-sudLwhcS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abFEp6Wa_E05"
      },
      "outputs": [],
      "source": [
        "# after researching \"pandas check for duplicates\" information on the following code was given\n",
        "# essentially, running an if true statement allows for a criteria to be filled or unfilled\n",
        "  # in this case, there either are duplicates or there arent duplicates\n",
        "\n",
        "lsoa_sample[\"LSOA11CD\"].duplicated().any()\n",
        "if lsoa_sample[\"LSOA11CD\"].duplicated().any() is True:\n",
        "  print (\"There are duplicated values.\")\n",
        "else:\n",
        "  print (\"There are no duplicated values.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6"
      ],
      "metadata": {
        "id": "22cEgpAIw2lV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qRsofxv4bXW"
      },
      "outputs": [],
      "source": [
        "# after reading the CSV file, the .plot() function is able to use categories such as the geometry in the table to create a plot of London with the LSOA boundaries\n",
        "\n",
        "lsoa_geo = gpd.read_file('/content/drive/MyDrive/Colab Notebooks/data1/londondata/Census20_LSOA.shp')\n",
        "lsoa_geo.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7"
      ],
      "metadata": {
        "id": "b614mS2zxvTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibxIZzbmAvjT"
      },
      "outputs": [],
      "source": [
        "# the .explore() method creates an adjustable, interactive map of the LSOA wards which allows for further spatial analysis\n",
        "# when dragging the mous over the wards, information from the original table is attributed to each individual geometric area throughout the map\n",
        "lsoa_geo.explore()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8"
      ],
      "metadata": {
        "id": "kmEqYkHkx1Ag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Ea0qHj5jM0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# similarly to the pandas exercises completed above, I filtered the data to only show areas with population counts over 1500 people\n",
        "\n",
        "new_sample = lsoa_sample[lsoa_sample[\"Pop20\"] > 1500]\n",
        "new_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.9"
      ],
      "metadata": {
        "id": "5iA336CUyCtz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvWR_Pu7A9ur"
      },
      "outputs": [],
      "source": [
        "new_sample.explore(column='Pop20', cmap='RdYlBu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.10"
      ],
      "metadata": {
        "id": "10mqKHSSzTo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kBIqJapBbN0"
      },
      "outputs": [],
      "source": [
        "# similar to above, I used the shape function to explore how many areas there are in the new dataset\n",
        "\n",
        "int(new_sample.shape[0])\n",
        "print(\"The number of areas is: \", int(new_sample.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.11"
      ],
      "metadata": {
        "id": "tTaCQPI3z5Kq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfb1tsLQBoX8"
      },
      "outputs": [],
      "source": [
        "# using the .sum() function, I calculated how many total people live in these areas\n",
        "\n",
        "int(new_sample[\"Pop20\"].sum())\n",
        "print(\"The total population of the subset layer is: \", int(new_sample[\"Pop20\"].sum()), \"people.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.12.01 - installs for Geopandas"
      ],
      "metadata": {
        "id": "Gk0fpHLY0lJy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtYLEnTqE0Zb"
      },
      "outputs": [],
      "source": [
        "pip install rasterio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.12"
      ],
      "metadata": {
        "id": "n48LazcB0TTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSvsh9VAA67z"
      },
      "outputs": [],
      "source": [
        "# these functions read the dataset as a rasterio dataset, meaning that you are preparing the future code to be able to properly read rasters\n",
        "\n",
        "import rasterio as rio\n",
        "from rasterio import plot\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/data_raster/Clipped_Raster.tif'\n",
        "\n",
        "elev = rio.open(file_path)\n",
        "elev"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.13"
      ],
      "metadata": {
        "id": "TECR-7a11Bhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu5XHmtyCzG1"
      },
      "outputs": [],
      "source": [
        "# since this raster is an elevation model, the elevation is defined as elev when rasterio reads the file path\n",
        "# in order to find the CRS of the dataset, we merely need to add .crs\n",
        "\n",
        "elev.crs\n",
        "print(\"The dataset of this raster is\", (elev.crs))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.14"
      ],
      "metadata": {
        "id": "hhCo6gka3IG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f05X2bqGSho"
      },
      "outputs": [],
      "source": [
        "# the raster bounds illustrate the area of ground that the raster covers\n",
        "\n",
        "print(elev.bounds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.15"
      ],
      "metadata": {
        "id": "esgYV0pf3Kmq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NarMMQX2Gl0R"
      },
      "outputs": [],
      "source": [
        "# the raster only has 1 band, meaning that it only contains a single layer of data\n",
        "# usually elevation models only have one band as they illustrate one criteria (elevation)\n",
        "\n",
        "print(elev.count)\n",
        "\n",
        "print(\"There are\", (elev.count), \"bands in this dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.16"
      ],
      "metadata": {
        "id": "-W-MGw-s3MF5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe06f9b6"
      },
      "outputs": [],
      "source": [
        "from rasterio import plot\n",
        "plot.show(elev)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.17"
      ],
      "metadata": {
        "id": "Gx8IZ3SL6d-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "elev_arr = elev.read(1)\n",
        "\n",
        "elev_masked = np.ma.masked_array(elev_arr, mask=(elev_arr == 0))\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 6]\n",
        "plt.hist(elev_masked.compressed(), bins=40, color='blue')\n",
        "plt.title(\"Histogram of Raster Values\")\n",
        "plt.xlabel(\"Pixel value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XtLEwdMQ54Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.18"
      ],
      "metadata": {
        "id": "DzWQffYD6tSl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDEJVfEqI7o8"
      },
      "outputs": [],
      "source": [
        "!pip install earthpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no way to create a False-Color Image, as this raster only has one band.\n",
        "\n",
        "False-Color rasters require three separate spectral bands (as we learned in SD2100: Sustainable Scotland during our NDVI analysis portion)\n",
        "\n",
        "Since our raster shape is (1,4546,7107), it contains only one band, meaning there is not NIR band, no Red band, and no Green band.\n",
        "\n",
        "EarthPy cannot generate a False-Color image out an input like (3, rows, columns)."
      ],
      "metadata": {
        "id": "HyPPe5Fp7URn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - K-Means and DBSCAN\n",
        "\n",
        "## **K-Means**: Part 4.0.1 to Part 4.14\n",
        "## **DBSCAN**: Part 4.15 to Part 4.29"
      ],
      "metadata": {
        "id": "ZAbGbp_s7i7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4.0.1 - installs for K-Means"
      ],
      "metadata": {
        "id": "CeN7stKD7uRH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT9dwkwNVbW-"
      },
      "outputs": [],
      "source": [
        "!pip install lonboard\n",
        "\n",
        "# when I tried to clear the cell outputs for this install, I was unable to visualize following map data, so I kept this cell in\n",
        "# other install cells have been cleared as they do not interfere with data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9WF4Re9VlFP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from lonboard import Map, ScatterplotLayer, viz\n",
        "import shapely"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OhN-v-rWY9w"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 and 4.2"
      ],
      "metadata": {
        "id": "Jh43uAo674t9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGqygU3jVqmW"
      },
      "outputs": [],
      "source": [
        "# pandas reads the file using the .read_csv function (5.1)\n",
        "\n",
        "acc = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/spatial_clustering/UK_Accident.csv\")\n",
        "\n",
        "# pandas displays the first 5 rows using the .head() function\n",
        "acc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3"
      ],
      "metadata": {
        "id": "NhuVLxoA8Vm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCwfha1DW26J"
      },
      "outputs": [],
      "source": [
        "# here, I kept columns that I deemed necessary, making sure to mix both categorical and numerical attributes\n",
        "\n",
        "keep = [\n",
        "    \"Accident_Severity\",\n",
        "    \"Longitude\",\n",
        "    \"Latitude\",\n",
        "    \"Road_Surface_Conditions\",\n",
        "    \"Day_of_Week\",\n",
        "    \"Number_of_Vehicles\",\n",
        "    \"Year\"\n",
        "]\n",
        "\n",
        "acc = acc[keep]\n",
        "acc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4"
      ],
      "metadata": {
        "id": "gfbdXneE8sAj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI6CTU3yXlhy"
      },
      "outputs": [],
      "source": [
        "# here, the instructions told us to slice the data and only analyse accidents from 2010 onward, hence why I put >= 2010 in the code\n",
        "\n",
        "acc_2010 = acc[acc[\"Year\"] >= 2010]\n",
        "acc_2010.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Co32v2k7ei-n"
      },
      "outputs": [],
      "source": [
        "# this code is confirmation of how many rows in the filtered dataset\n",
        "\n",
        "acc_2010 = acc_2010[acc_2010[\"Year\"] >= 2010]\n",
        "print(\"Rows in 2010 dataset:\", len(acc_2010))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5"
      ],
      "metadata": {
        "id": "8WRGSYNt89i4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfwcLuSGfDuA"
      },
      "outputs": [],
      "source": [
        "# using previous knowledge on how to create a histogram, I created a graph to illustrate what day of the week results in the most accidents\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(\n",
        "    data=acc_2010,\n",
        "    x=\"Day_of_Week\",\n",
        "    order=sorted(acc_2010[\"Day_of_Week\"].unique())\n",
        ")\n",
        "plt.title(\"Number of Accidents per Day of the Week - 2010 onwards\")\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Number of Accidents\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The day with the most accidents is attributed to the 'highest' plot, in this case being Saturday."
      ],
      "metadata": {
        "id": "SUqsE9qz9V-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6"
      ],
      "metadata": {
        "id": "mGojqPjc9c9g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDVcv30Of_iY"
      },
      "outputs": [],
      "source": [
        "# This plot illustrates the relationship between Road Surface Conditions and Accident Severity\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(\n",
        "    data=acc_2010,\n",
        "    x=\"Road_Surface_Conditions\",\n",
        "    hue=\"Accident_Severity\"\n",
        ")\n",
        "plt.xticks\n",
        "plt.title(\"Accident Severity by Road Surface Conditions - 2010 onwards\")\n",
        "plt.xlabel(\"Road Surface Conditions\")\n",
        "plt.ylabel(\"Number of Accidents\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This histogram illustrates that proportionally, icy or wet conditions have more severe accidents.\n",
        "\n",
        "However, most accidents happen on dry roads due to large traffic volumes on highways and other densely-trafficked, higher elevation roads. In this dataset, dry roads are more frequent which results in more overall accidents happening on that road type.\n",
        "\n",
        "Environmental conditions can still impact the severity and frequency of car accidents.\n",
        "\n",
        "It is reasonable to assume that more car accidents happen on Saturdays because more individuals have free time to travel, run errands, and use their cars than they do during the work week."
      ],
      "metadata": {
        "id": "rW1nnFWw9m0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7"
      ],
      "metadata": {
        "id": "n45GJVyC-L6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN4-g7ZchUeq"
      },
      "outputs": [],
      "source": [
        "geometry = gpd.points_from_xy(acc_2010[\"Longitude\"], acc_2010[\"Latitude\"])\n",
        "gdf_accidents_2010 = gpd.GeoDataFrame(acc_2010, geometry=geometry, crs=\"EPSG:4326\")\n",
        "gdf_accidents_2010.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDAwGmv2iWl6"
      },
      "outputs": [],
      "source": [
        "# the viz function visualises the dataset\n",
        "viz(gdf_accidents_2010)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8"
      ],
      "metadata": {
        "id": "qrowH5ux-Y-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwV6-4wzit2C"
      },
      "outputs": [],
      "source": [
        "# this illustates the total boundaries for all accidents in the dataset shown above\n",
        "gdf_accidents_2010.total_bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb118jFWjCDL"
      },
      "outputs": [],
      "source": [
        "# however, we want to convey only accidents in the Glasgow/Edinburgh region\n",
        "# by figuring out the bounday box for the region, we can input those points (below) into a visualization of the data\n",
        "# ge_bbox = Glasgow Edinburgh Boundary Box\n",
        "ge_bbox = [-4.6, 55.74, -3.0, 56.05]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmqDGca8j4cS"
      },
      "outputs": [],
      "source": [
        "# in order to visualze data from the new filtered area, we have to assess the new shape of our boundary box with the formula below\n",
        "\n",
        "gdf_scot = gdf_accidents_2010[gdf_accidents_2010.intersects(shapely.box(*ge_bbox))]\n",
        "gdf_scot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqv-fxm2kEzD"
      },
      "outputs": [],
      "source": [
        "# this is a visualization of the accidents filtered only to the Glasow and Edinburgh area\n",
        "\n",
        "viz(gdf_scot)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9"
      ],
      "metadata": {
        "id": "Quaz_v6Z_XJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n6Skjq9kUgz"
      },
      "outputs": [],
      "source": [
        "# Here, I have implemented K-Means for a value of 3 to the Glasgow/Edinburgh Dataset\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import lonboard as ld\n",
        "\n",
        "kmeans_3 = KMeans(n_clusters=3, random_state=42)\n",
        "gdf_scot['kmeans_3'] = kmeans_3.fit_predict(gdf_scot[['Longitude', 'Latitude']])\n",
        "gdf_scot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RllHY5QbXtF-"
      },
      "outputs": [],
      "source": [
        "# Here, I have done the same work as above but insteaad changed the K-Means value to 5\n",
        "\n",
        "kmeans_5 = KMeans(n_clusters=5, random_state=42)\n",
        "gdf_scot['kmeans_5'] = kmeans_5.fit_predict(gdf_scot[['Longitude', 'Latitude']])\n",
        "gdf_scot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.10"
      ],
      "metadata": {
        "id": "Gr-x638ZUPEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXcRv2yJYeGl"
      },
      "outputs": [],
      "source": [
        "# this code creates colors for each K-Means cluster and gives a certain color to every point\n",
        "# since we are working with K-Means = 3, which does not have much varying data in it, most likely only one color is going to be assigned to the clusters\n",
        "# Latitude and Longitude are not compared with other criteria, meaning that this K-Means visualization will show clusters where accidents most frequently happen in certain Lat/Lon areas within the new dataset\n",
        "categories = gdf_scot['kmeans_3'].unique()\n",
        "colors = sns.color_palette(\"bright\", len(categories))\n",
        "color_dict = dict(zip(categories, colors))\n",
        "\n",
        "color_array = np.array([\n",
        "    np.append(\n",
        "        np.multiply(color_dict.get(x, (0, 0, 0)), 255).astype(\"uint8\"),\n",
        "        200\n",
        "    )\n",
        "    for x in gdf_scot['kmeans_3']\n",
        "], dtype=\"uint8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6grQGrUZAz1"
      },
      "outputs": [],
      "source": [
        "# this maps the clusters, and as we can see they are all only 1 color (red)\n",
        "\n",
        "layer = ScatterplotLayer.from_geopandas(gdf_scot)\n",
        "layer.radius_scale = 40\n",
        "layer.opacity = 0.1\n",
        "layer.get_fill_color = [255, 0, 0, 200]\n",
        "\n",
        "map_clusters = Map(layers=[layer], height=500)\n",
        "map_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDlZD7geMtX"
      },
      "outputs": [],
      "source": [
        "# for the K-Means 5 map, I manually defined th cluster colors so I knew what would be displayed in the visualization below\n",
        "\n",
        "cluster_colors = {\n",
        "    0: [255,   0,   0, 180],   # red\n",
        "    1: [  0, 255,   0, 180],   # green\n",
        "    2: [  0,   0, 255, 180],   # blue\n",
        "    3: [255, 165,   0, 180],   # orange\n",
        "    4: [128,   0, 128, 180]    # purple\n",
        "}\n",
        "\n",
        "layers = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSnY9RuBePlf"
      },
      "outputs": [],
      "source": [
        "# this ensures that clusters only show points that specifically belong to tha cluster (no outliers)\n",
        "\n",
        "for cluster_value in sorted(gdf_scot['kmeans_5'].unique()):\n",
        "    gdf_subset = gdf_scot[gdf_scot['kmeans_5'] == cluster_value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMbyWpWdeUP_"
      },
      "outputs": [],
      "source": [
        "# once again, this shows how to create a scatterplot\n",
        "\n",
        "layer = ScatterplotLayer.from_geopandas(gdf_subset)\n",
        "layer.radius_scale = 40\n",
        "layer.opacity = 1.0\n",
        "layer.get_fill_color = cluster_colors[cluster_value]\n",
        "layers.append(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkxlnbGofF1C"
      },
      "outputs": [],
      "source": [
        "map_k5 = Map(layers=layers, height=500)\n",
        "map_k5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.11"
      ],
      "metadata": {
        "id": "AB8RGJ3EUa27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When applying the K-means clustering to the >=2010 dataset, the map illustrates accident areas in spatial clusters based on geographic coordinates.\n",
        "\n",
        "The center point of a cluster is defined as a 'centroid', where the average location of all associated accident points are assigned (i.e., a mean vlue of accidents in a certain geographic area are assigned to one centorid)\n",
        "Additionally, the centroid is defined at the average latitude and longitude of accidents in that cluster\n",
        "\n",
        "When assigning new/updated points to a dataset, the K-means recaclulates the centroids to their new average/respective positions and reassesses where points lay on the map.\n",
        "\n",
        "K = 3 cluster:\n",
        "\n",
        "*   The algorithm is more broad in this cluster, as it covers a much larger area between Glasgow and Edinburgh, focusing on the two cities' densely urban areas and central bands (roads) between the two points (roadways with higher chance of accidents happening)\n",
        "\n",
        "*   Essentially, when k is lower, there is a wider scale spatial difference betwen the points, and they are bound to be less specific to each individual accident and more focused on the larger sapatial assessment of the accident-prone areas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "K = 5 cluster:\n",
        "  * Because the k value is higher in this map projection, the cluster is more concentrated to a specific area, in this case being a sub-cluster of Glasgow, which includes the city center and surrounding suburbian areas\n",
        "  * This map is useful for assessing accidents on a city-wide scale, as the algorithm contains more centroids and can assess smaller spatial patterns within an area.\n",
        "  * This gives a more geographically accurate depiction of locations of car accidents within the Glasgow area\n",
        "\n",
        "As k increases:\n",
        "  * Clusters ultimately become more specific, as the k value covers a smaller geographic area\n",
        "  * Centroids more accurately reflect the mean location of these accidents\n",
        "  * Subtle patterns within the dataset (such as common locations for car accidents) become more visible within a smaller area\n",
        "\n",
        "As k decreases:\n",
        "  * Clusters merge with each other to centroids and form larger pockets of data, which is useful for a general assessment of car accident location density over a large area of land\n",
        "  * This can highlight longer term, national spatial trends in assessing accident-prone areas, and can allow for policymakers to visualize large-scale issues within road safety policy\n"
      ],
      "metadata": {
        "id": "Mk-oXExSCW3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.12"
      ],
      "metadata": {
        "id": "-R2nu07rDA75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQK_mHf0oKGl"
      },
      "outputs": [],
      "source": [
        "# defines new criteria we will be working with -- in this case we are comparing Accident Severity with Nuber of Vehicles involved\n",
        "\n",
        "X = gdf_scot[['Accident_Severity', 'Number_of_Vehicles']]\n",
        "\n",
        "kmeans_attributes = KMeans(n_clusters=3, random_state=42)\n",
        "\n",
        "gdf_scot['k_means_attrib_3'] = kmeans_attributes.fit_predict(X)\n",
        "gdf_scot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XlgxVVnpOQV"
      },
      "outputs": [],
      "source": [
        "# Same as above, this time with K-Means 5\n",
        "\n",
        "kmeans_attributes_5 = KMeans(n_clusters=5, random_state=42)\n",
        "gdf_scot['k_means_attrib_5'] = kmeans_attributes_5.fit_predict(X)\n",
        "gdf_scot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.13"
      ],
      "metadata": {
        "id": "bRkafRNtUvF8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXX6iHYop8tR"
      },
      "outputs": [],
      "source": [
        "# similar process as the K-Means for Latitude and Longitude seen above, but now we are comparing different criteria\n",
        "\n",
        "attr_cluster_colors = {\n",
        "    0: [255, 0, 0, 180],      # red\n",
        "    1: [0, 255, 0, 180],      # green\n",
        "    2: [0, 0, 255, 180]       # blue\n",
        "}\n",
        "\n",
        "layers_attr = []\n",
        "\n",
        "for cluster_value in sorted(gdf_scot['k_means_attrib_3'].unique()):\n",
        "  gdf_subset = gdf_scot[gdf_scot['k_means_attrib_3'] == cluster_value]\n",
        "\n",
        "layer = ScatterplotLayer.from_geopandas(gdf_subset)\n",
        "layer.radius_scale = 40\n",
        "layer.opacity = 1.0\n",
        "layer.get_fill_color = attr_cluster_colors[cluster_value]\n",
        "layers_attr.append(layer)\n",
        "\n",
        "map_kattrs3 = Map(layers=layers_attr, height=500)\n",
        "map_kattrs3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h8VCCgirTEr"
      },
      "outputs": [],
      "source": [
        "# This scatterplot looks different, as it has multiple colors on display.\n",
        "# This is analysed in a text cell below the map.\n",
        "\n",
        "attr_cluster_colors = {\n",
        "    0: [255, 0, 0, 180],\n",
        "    1: [0, 255, 0, 180],\n",
        "    2: [0, 0, 255, 180],\n",
        "    3: [255, 165, 0, 180],\n",
        "    4: [128, 0, 128, 180]\n",
        "}\n",
        "\n",
        "layers_attr = []\n",
        "\n",
        "for cluster_value in sorted(gdf_scot['k_means_attrib_5'].unique()):\n",
        "    gdf_subset = gdf_scot[gdf_scot['k_means_attrib_5'] == cluster_value]\n",
        "\n",
        "    layer = ScatterplotLayer.from_geopandas(gdf_subset)\n",
        "    layer.radius_scale = 40\n",
        "    layer.opacity = 1.0\n",
        "    layer.get_fill_color = attr_cluster_colors[int(cluster_value)]\n",
        "\n",
        "    layers_attr.append(layer)\n",
        "\n",
        "map_kattrs5 = Map(layers=layers_attr, height=500)\n",
        "map_kattrs5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.14"
      ],
      "metadata": {
        "id": "nppsh4caUrbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The coordinate-only structure (e.g., k_means_3 and k_means_5) illustrated only spatial structure, as the clustering relied only on location.\n",
        "    * The clustering highlighted areas in which accidents most frequently occurred throughout both the Glasgow/Edinburgh area (k_means_3) and the Glasgow city/suburban area (k_means_5).\n",
        "    * These clusters focused on WHERE accidents most commonly happen, and offer insight into geographic hotspots or distinct & repeating areas of accident density.\n",
        "\n",
        "* The attribute-based clusters group accidents based on specific characteristics as seen in the table (in this case: Accident Severity and Number of Vehicles).\n",
        "    * The map illustrates how these clusters are distributed across space, and certain classifications for accidents can emerge (e.g., low severity versus high-profile accidents) that may be more common on certain types of transport routes.\n",
        "\n",
        "* For k = 3:\n",
        "    * The attribute data was not separated into multiple groups, as the two categories did not vary greatly enough to create distinct classifications over the area.\n",
        "        * Essentially, the color of the centroids on the map was only one color (blue), as there was not enough variation between the two fields to create a visible distinction.\n",
        "        * Because K-Means relies on differences in variables, the algorithm was unable to distinguish a clear boundary between groups within the cluster, meaning that the model collapses into a dominant clustered map.\n",
        "            * After running the code (kmeans_attributes.cluster_centers_), I assessed the meaning of the dominant color (blue):\n",
        "                * Cluster 0  Blue: represents medium-high severity accidents (K-Means ~2.89), with standard two-car collisions (K-Means 2).\n",
        "                    * This represents typical moderately high-severity crashes on urban and suburban roads.\n",
        "\n",
        "* For k = 5:\n",
        "    * The region for k = 5 was divided into smaller, individually colored groups based on variation between the variables, which is useful for assessing differences in accident characteristics across space.\n",
        "    * These clusters now reflect the subtle differences that appear when using more centroids (increasing the k value).\n",
        "        * For example, the five clusters (colors) illustrate traits like increased severity scores or a higher count of vehicles involved in accidents relative to each centroid.\n",
        "        * By clicking on an individual point on the map, a table appears with the K-Means number for both k = 5 and k = 3.\n",
        "            * After running the code (kmeans_attributes_5.cluster_centers_), I assessed the meaning of each color/cluster:\n",
        "                * Cluster 0  Red: represents high severity accidents (K-Means 3) that likely involve 2 vehicles (K-Means 2).\n",
        "                * Cluster 1  Green: represents high severity accidents (K-Means 3) that likely involve only one vehicle (K-Means 1).\n",
        "                * Cluster 2  Blue: represents relatively high severity accidents (K-Means ~2.97) that involve an average of 3.3 vehicles (K-Means ~3.3).\n",
        "                * Cluster 3  Orange: represents medium severity accidents (K-Means ~1.94) that involve an average of 2.1 vehicles (K-Means ~2.1).\n",
        "                * Cluster 4  Purple: represents medium/low severity accidents (K-Means ~1.93) that involve only one vehicle (K-Means 1).\n",
        "    * In conclusion, minor differences in these accident characteristics can reflect certain geographical, environmental, or transport-specific issues in the area's infrastructure.\n",
        "\n",
        "* It is important to confirm that attributes have enough variability between each other to display distinct and differentiating values (or increase the K-Means) so that variability can be assessed and used in preventing further accidents.\n",
        "\n",
        "* Spatial clustering is more helpful in assessing where geographically accidents are more likely to occur within a certain area.\n"
      ],
      "metadata": {
        "id": "SQRWQbUWEdZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.15 - Begin of DBSCAN Part of the Exercise"
      ],
      "metadata": {
        "id": "VGiotNFaFBio"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYzbpkU1ubqO"
      },
      "outputs": [],
      "source": [
        "# here I  have created a new Dataframe as to not be confused with the previous, filtered one and named it gdf_dbscan for the purpose of the following steps\n",
        "\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "\n",
        "gdf_dbscan = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/spatial_clustering/UK_Accident.csv\")\n",
        "\n",
        "gdf_dbscan[['Longitude', 'Latitude']].head()\n",
        "\n",
        "gdf_dbscan = gdf_dbscan.dropna(subset=['Longitude', 'Latitude'])\n",
        "\n",
        "geometry = gpd.points_from_xy(gdf_dbscan['Longitude'], gdf_dbscan['Latitude'])\n",
        "\n",
        "gdf_dbscan = gpd.GeoDataFrame(gdf_dbscan, geometry=geometry, crs=\"EPSG:4326\")\n",
        "gdf_dbscan.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.16"
      ],
      "metadata": {
        "id": "OJFxmTPoFoAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBbcunDpwJcT"
      },
      "outputs": [],
      "source": [
        "# using the website above, I found the Boundary Box for the greater Birmingham area\n",
        "# I chose to include the larger metropolitan area as opposed to the city proper in hopes that using a larger location could illustrate some accident patterns\n",
        "\n",
        "bham_bbox = [-2.30, 52.30, -1.45, 52.70]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shapely\n",
        "\n",
        "bham_bbox = (-2.30, 52.30, -1.45, 52.70)\n",
        "\n",
        "bbox_shape = shapely.box(*bham_bbox)\n",
        "\n",
        "gdf_bham = gdf_dbscan[gdf_dbscan.intersects(bbox_shape)]"
      ],
      "metadata": {
        "id": "VvJ6bqyrHZXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.17"
      ],
      "metadata": {
        "id": "FDLcj3W3U-DM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNh0lH8ayIAc"
      },
      "outputs": [],
      "source": [
        "from lonboard import Map, ScatterplotLayer\n",
        "\n",
        "layer_bham = ScatterplotLayer.from_geopandas(gdf_bham)\n",
        "layer_bham.radius_scale = 40\n",
        "layer_bham.opacity = 1.0\n",
        "layer_bham.get_fill_color = [0, 0, 255, 180]  # blue points\n",
        "\n",
        "map_bham = Map(layers=[layer_bham], height=500)\n",
        "map_bham"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.18"
      ],
      "metadata": {
        "id": "H612KQTyPfVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aFMCdKFyy8i"
      },
      "outputs": [],
      "source": [
        "# numerical attributes are categorized as int64 or float64 - referring to numerical output\n",
        "# catgorical attributes are categorized as object - referring to categories that cannot be explicitly defined numerically (e.g., road type or weather conditions)\n",
        "# geometry is defined as itself because it is its own category containing the points of the attributes\n",
        "\n",
        "gdf_bham.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.19"
      ],
      "metadata": {
        "id": "FZJCTyhMPok6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3Z1vLTt0a9J"
      },
      "outputs": [],
      "source": [
        "corr = gdf_bham.corr(numeric_only=True)\n",
        "corr\n",
        "\n",
        "# when running (numeric_only=True) this runs the correlation on only numerical categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brX6rBa-05-J"
      },
      "outputs": [],
      "source": [
        "# the part of the code that needed changing was 'your_correlaion_variable' to 'corr' as we defined it in one of the preceding cells as: corr = gdf_bham.corr(numeric_only=True)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr,\n",
        "    annot=True,\n",
        "    cmap='coolwarm',\n",
        "    fmt='.2f',\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'label':'Correlation Coefficient'}\n",
        ")\n",
        "\n",
        "plt.title('Pearson Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.20"
      ],
      "metadata": {
        "id": "eHJJ_7mkRcsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApkCXuEg2W8X"
      },
      "outputs": [],
      "source": [
        "!pip install pysal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.21"
      ],
      "metadata": {
        "id": "_pfv05kPRvuL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugJPMM8X2NbQ"
      },
      "outputs": [],
      "source": [
        "import libpysal.weights as weights\n",
        "from esda.moran import Moran"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.22"
      ],
      "metadata": {
        "id": "1FqoU3cYRxOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rucLD-Hb2kVR"
      },
      "outputs": [],
      "source": [
        "gdf_bham_reproj = gdf_bham.to_crs(epsg=27700)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.23"
      ],
      "metadata": {
        "id": "ZrJer6IzVZaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha61IlFBD20L"
      },
      "outputs": [],
      "source": [
        "w = weights.DistanceBand.from_dataframe(\n",
        "    gdf_bham_reproj,\n",
        "    threshold=500,\n",
        "    ids=gdf_bham_reproj.index,\n",
        "    silence_warnings=True\n",
        ")\n",
        "w.transform = 'R'\n",
        "\n",
        "moran = Moran(\n",
        "    gdf_bham_reproj['Accident_Severity'],\n",
        "    w\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Moran's I Spatial Autocorrelation Analysis ---\")\n",
        "print(f\"Defined {w.n} observations and {w.mean_neighbors:.2f} average neighbors per point.\")\n",
        "print(f\"\\nMoran's I Statistic (Observed I): {moran.I:.4f}\")\n",
        "print(f\"P-value (significance): {moran.p_sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.24"
      ],
      "metadata": {
        "id": "tffFpOTrVfm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Correlation Matrix provides a visual for assessing how certain attributes rely on each other to 'come to fruition' in the sense that one attribute potentially causes the other to happen, or one can explain/reason for why the other occurs\n",
        "In some Correlation Indexes, there are categories that are not mutually exclusive, meaning that they one explicitly causes the other\n",
        "  * In this Correlation Matrix, the Accident_Severity numerical category has a very weak relationship with all other attributes\n",
        "     * this suggests that the Accident_Severity attribute portrayed in the numerical dataset is not driven by a dominant condiiton (i.e., year, day of the week, or road classification)\n",
        "  * On the contrary, the strongest correlation between attributes (that dont relate to location) in the Correlation Index is the relationship between Speed_Limit and Urban_or_Rural_Area with a Correlation Coefficient of 0.67\n",
        "    * This can be used to make the claim that increased speeds on varying road types have an impact on accident frequency.\n",
        "      * when someone is going at maximum speeds on smaller roads, they are more likely to be reckless and cause an accident with cars that adhere to the speed limit.\n",
        "      * In turn, cars that go far under the speed limit on major roads and highways are much more likely to be the cause for accident as other drivers may adhere to the higher speed limit.\n",
        "        * It is absolutely crucial to follow speed limits and never stray too far under or over the assigned speed.\n",
        "\n",
        "With reference to morans-I, the spatial perspective of the reprojected Birmingham Dataframe can be assessed\n",
        "  * the resulting value for morans-I is 0.0124 with a p-value of 0.0010\n",
        "    * The morans-I value is very close to 0, meaning there is little-to-no spatial clustering or 'severe accident hotspots' within the area. The severity of accidents is seemingly random with no areas being standouts for severe accidents.\n",
        "    * Even though there is almost no spatial correlation, the low p-value of 0.0010 still proves to be statistically significant and valid\n",
        "\n",
        "In conclusion, there is very smal, but nonetheless, technically statistically signifigant relationships between some attributes, but accident severity does not depend on the other numerical values in the reprojected dataset\n",
        "  * This implies that severity in accidents is moreso influenced by sitational factors on the day-of (e.g., weather conditions at the time of accident, road conditions, driver impairment) as opposed to assessable spatial patterns"
      ],
      "metadata": {
        "id": "epM0l4AlSASN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.25.01 - installs/reprojection for part B of DBSCAN"
      ],
      "metadata": {
        "id": "bCX9xF6USscI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU6cY7Ha_WW6"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = gdf_bham_reproj[['Location_Easting_OSGR', 'Location_Northing_OSGR']]\n",
        "\n",
        "dbscan = DBSCAN(eps=50, min_samples=15)\n",
        "gdf_bham_reproj['dbscan_labels'] = dbscan.fit_predict(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WfmgeFT_Y9a"
      },
      "outputs": [],
      "source": [
        "gdf_bham['dbscan_labels'] = gdf_bham_reproj['dbscan_labels'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.25"
      ],
      "metadata": {
        "id": "UiFw7Jr_V0ph"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOIZwuRQyjot"
      },
      "outputs": [],
      "source": [
        "# DBSCAN Clustering of Road Traffic Accidents (eps = 200, min_samples = 40)\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = gdf_bham_reproj[['Location_Easting_OSGR', 'Location_Northing_OSGR']]\n",
        "\n",
        "dbscan = DBSCAN(eps=200, min_samples=40)\n",
        "gdf_bham_reproj['dbscan_labels'] = dbscan.fit_predict(X)\n",
        "\n",
        "gdf_bham['dbscan_labels'] = gdf_bham_reproj['dbscan_labels'].values\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf_bham,\n",
        "    lat=gdf_bham.geometry.y,\n",
        "    lon=gdf_bham.geometry.x,\n",
        "    color=gdf_bham['dbscan_labels'],\n",
        "    zoom=9,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.update_layout(mapbox_style=\"carto-positron\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4-XFQ0Nyjwq"
      },
      "outputs": [],
      "source": [
        "# DBSCAN Clustering of Road Traffic Accidents (eps = 200, min_samples = 30)\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = gdf_bham_reproj[['Location_Easting_OSGR', 'Location_Northing_OSGR']]\n",
        "\n",
        "dbscan = DBSCAN(eps=200, min_samples=30)\n",
        "gdf_bham_reproj['dbscan_labels'] = dbscan.fit_predict(X)\n",
        "\n",
        "gdf_bham['dbscan_labels'] = gdf_bham_reproj['dbscan_labels'].values\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf_bham,\n",
        "    lat=gdf_bham.geometry.y,\n",
        "    lon=gdf_bham.geometry.x,\n",
        "    color=gdf_bham['dbscan_labels'],\n",
        "    zoom=9,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.update_layout(mapbox_style=\"carto-positron\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rZ3KNtsy-HG"
      },
      "outputs": [],
      "source": [
        "# DBSCAN Clustering of Road Traffic Accidents (eps = 200, min_samples = 15)\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = gdf_bham_reproj[['Location_Easting_OSGR', 'Location_Northing_OSGR']]\n",
        "\n",
        "dbscan = DBSCAN(eps=200, min_samples=15)\n",
        "gdf_bham_reproj['dbscan_labels'] = dbscan.fit_predict(X)\n",
        "\n",
        "gdf_bham['dbscan_labels'] = gdf_bham_reproj['dbscan_labels'].values\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf_bham,\n",
        "    lat=gdf_bham.geometry.y,\n",
        "    lon=gdf_bham.geometry.x,\n",
        "    color=gdf_bham['dbscan_labels'],\n",
        "    zoom=9,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.update_layout(mapbox_style=\"carto-positron\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzNmhgmC_chy"
      },
      "outputs": [],
      "source": [
        "# DBSCAN Clustering of Road Traffic Accidents (eps = 50, min_samples = 15)\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = gdf_bham_reproj[['Location_Easting_OSGR', 'Location_Northing_OSGR']]\n",
        "\n",
        "dbscan = DBSCAN(eps=50, min_samples=15)\n",
        "gdf_bham_reproj['dbscan_labels'] = dbscan.fit_predict(X)\n",
        "\n",
        "gdf_bham['dbscan_labels'] = gdf_bham_reproj['dbscan_labels'].values\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf_bham,\n",
        "    lat=gdf_bham.geometry.y,\n",
        "    lon=gdf_bham.geometry.x,\n",
        "    color=gdf_bham['dbscan_labels'],\n",
        "    zoom=9,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.update_layout(mapbox_style=\"carto-positron\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83VBc6Hxt2HY"
      },
      "outputs": [],
      "source": [
        "# DBSCAN Clustering of Road Traffic Accidents (eps = 500, min_samples = 15)\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = gdf_bham_reproj[['Location_Easting_OSGR', 'Location_Northing_OSGR']]\n",
        "\n",
        "dbscan = DBSCAN(eps=500, min_samples=15)\n",
        "gdf_bham_reproj['dbscan_labels'] = dbscan.fit_predict(X)\n",
        "\n",
        "gdf_bham['dbscan_labels'] = gdf_bham_reproj['dbscan_labels'].values\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf_bham,\n",
        "    lat=gdf_bham.geometry.y,\n",
        "    lon=gdf_bham.geometry.x,\n",
        "    color=gdf_bham['dbscan_labels'],\n",
        "    zoom=9,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.update_layout(mapbox_style=\"carto-positron\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqoLOlP5usIi"
      },
      "outputs": [],
      "source": [
        "# DBSCAN Clustering of Road Traffic Accidents (eps = 200, min_samples = 15)\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X = gdf_bham_reproj[['Location_Easting_OSGR', 'Location_Northing_OSGR']]\n",
        "\n",
        "dbscan = DBSCAN(eps=200, min_samples=15)\n",
        "gdf_bham_reproj['dbscan_labels'] = dbscan.fit_predict(X)\n",
        "\n",
        "gdf_bham['dbscan_labels'] = gdf_bham_reproj['dbscan_labels'].values\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter_mapbox(\n",
        "    gdf_bham,\n",
        "    lat=gdf_bham.geometry.y,\n",
        "    lon=gdf_bham.geometry.x,\n",
        "    color=gdf_bham['dbscan_labels'],\n",
        "    zoom=9,\n",
        "    height=700,\n",
        ")\n",
        "\n",
        "fig.update_layout(mapbox_style=\"carto-positron\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkwUxjka907W"
      },
      "outputs": [],
      "source": [
        "gdf_bham['dbscan_labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.27"
      ],
      "metadata": {
        "id": "lrQw7FRbWiBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**min_sample (all eps = 200 in these cases)**\n",
        "\n",
        "Running the DBSCAN with a min_sample = 15 and the eps = 200 identified varying clusters across the larger Birmingham area in addition to a substantial amount of 'noise' or an accident point that doesn't belong to a cluster in the plot\n",
        "  * this can happen when there is an accident that is not close enough to neighboring accidents wihin the eps = 200 (distance) or when the individual accident is far from denser accident areas/hotspots\n",
        "  * it is also seen that the 'noise' (-1) value is seen frequently on roads and highways, which is expected when accidents occur on longer stretches of road, but it can be seen that more hotspots form at intersecting highways, exits, and merging lanes\n",
        "After analysis and running the DBSCAN with varying min_sample values (40,30,15), I have come to the conclusion that lower min_sample values in a DBSCAN make it easier for assessing accidents in clusters as the creation of a cluster requires only 15 nearby accidents as opposed to 30 or 45\n",
        "  * On the contrary, if one is attempting to assess wider accident patterns (i.e., clusters of 50+ cars forming large hotspots over time), one should up the min_sample value to a higher number to assess long term spatial patterns\n",
        "\n",
        "\n",
        "**eps (all min_sample = 15 in these cases)**\n",
        "\n",
        " eps controls how close points need to be in order to be considered neighbors (measured in meters).\n",
        "  * After running multiple DBSCANs with varying eps (50m, 200m, 500m), I have analysed that when eps is lower (i.e., 50), only very small accident hotspots are detected, and primarily, they arise in the city center and at highway intersections/lane changes\n",
        "    * This can be useful in assessing major accident hotspots and revisiting traffic legisliation or existing infrastructure in the city at these points to make Birmingham much more safe\n",
        "  * After running a DBSCAn with an eps of 500, the clusters become much larger, and accidents that are up to 500m apart are now considered neighbors. Additinoally, separate clusters combine into much larger hotspots and the noise (-1) significantly decreases.\n",
        "    * This eps is less heloful than a smaller one as it poorly illustrates the hotspots and condenses accident areas into too large of a distance\n",
        "  * After running the DBSCAN with an eps of 200, I concluded that this eps would be most useful in assessing all types of accident clusters, as it illustrated both significant amounts of noise and also illustrated varying accident cluster types, ranging from less to more frequent areas\n",
        "    * This can be helpful in assessing how generous the DBSCAN must be in grouping points, as analysing varying regions throughout Birmingham is important to undrstanding potential underlying problems that cause accidents in the area\n"
      ],
      "metadata": {
        "id": "bAL4IAdkWB3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.28"
      ],
      "metadata": {
        "id": "uPmBbgSaWlfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing K-Means with DBSCAN**\n",
        "\n",
        "  * K-Means and DBSCAN work in very different ways, as K-Means starts with a set number of clusters and delegates accidents into a fixed area (not dependent on distance from the cluster like DBSCAN)\n",
        "    * K-Means is very useful for assessing spatial inequalities with the accidents as it gives a clearer picture of the location in which these accidents generally occur\n",
        "    * K-Means does not offer much insight into accident hotspots, as it potentially groups accidents in clusters witch which they are not spatially close to\n",
        "\n",
        "  * DBSCAN focuses on accident density, and is crucial for assessing accident hotspots in an area (as seen above) it is extremely helpful in understanding where accidents most frequenly occur over a period of time\n",
        "    * Adjusting the eps and min_samples values was also crucial to understanding the data, as a lower min_samples value (i.e., 15) illustrated smaller clusters that were not visible with a larger min_samples value\n",
        "    * This is important for addressing isues in infrastructure on roadways that may be causing these accidents, for example some roadways that have a large hotspot of accidents may need to be repaved or iced more frequently during winter to prevent further accidents\n",
        "\n",
        "Overall, while K-Means is useful for assessing general clusters acrross a given area, it can hide underlying paterns of a local area. Additionally, while DBSCAN prioritizes focus on precise hospots and clusters, it is variable by its eps and min_samples values, so it is important to adjust those according to the data\n",
        "  Both methods used together can paint a clear picture of accident data in Birmingham\n"
      ],
      "metadata": {
        "id": "MAqWZGUlWm3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.29"
      ],
      "metadata": {
        "id": "HF4x1uJYXER9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As I stated above, DBSCAN is incredibly useful in presenting to policymakers for the purpose of addressing potentially faulty infrastructure and traffic plans.\n",
        "Large hotspots and denser cluster areas may illustrate isues of inadequate road signs (speed limits, construction, hazards) and it would benefit residents of Birmingham to have these issued addressed.\n",
        "\n",
        "Additionally, Birmingham experiences cold weather in the winter, and icy roads at merge points on highways pose severe safety hazards to drivers. Assessing a K-means plot in conjunction with a DBSCAN would illustrate comparisons between weather conditions and accident severity, while also assessing accident hotspots\n",
        "   * This could allow road workers to put rock salt on areas of the road before it freezes, as a preemptive measure to reduce accidents\n",
        "The smaller clusters may illustrate mor eminute issues of infrastructure like short merging lanes, easily 'jammable' traffic areas, and narrow roads\n",
        "  * While assessing all of these infrastructure issues is timely and costly, both the DBSCAN and K-Means maps provide meaningful insight on how to make roads safer for residents in the greater Birmingham area, starting with the locations that need it most."
      ],
      "metadata": {
        "id": "J_qIGUnPXFym"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-cXOryPZ5rmE",
        "euiqaQRyRtUB",
        "WSz0StQnuAcD",
        "MGAqyiXAuRP2",
        "U2zePclpz52b",
        "h206QDqMvS39",
        "sIQ6MvfFvwod",
        "fxbZMe9_wCBQ",
        "5Ym-sudLwhcS",
        "22cEgpAIw2lV",
        "b614mS2zxvTZ",
        "kmEqYkHkx1Ag",
        "5iA336CUyCtz",
        "10mqKHSSzTo3",
        "tTaCQPI3z5Kq",
        "Gk0fpHLY0lJy",
        "n48LazcB0TTy",
        "TECR-7a11Bhl",
        "hhCo6gka3IG5",
        "esgYV0pf3Kmq",
        "-W-MGw-s3MF5",
        "CeN7stKD7uRH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}